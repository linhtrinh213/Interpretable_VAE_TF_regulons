{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNnDcW7xRxEjbQEFCOSpu1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhtrinh213/Interpretable_VAE_TF_regulons/blob/main/Stochastic_Weight_Averaging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stochastic Weight Averaging\n",
        "- Implementing SWA on our Variational Autoencoder by wrapping Adam optimizer using SWA class, and then train model. After training set the weights of the model to the SWA averages.\n",
        "\n",
        "- SWA works by averaging model weights collected during training with stochastic gradient descent (SGD), which typically converges near the edges of low-loss regions. These edge solutions often generalize poorly to test data. In contrast, the averaging process in SWA tends to produce solutions located at the center of wide, flat regions in the loss landscape, which are known to generalize better. During the final 25% of training, the learning rate is increased to encourage exploration of the low-loss region before averaging begins. More on the method on: https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/"
      ],
      "metadata": {
        "id": "sBfgfG8B4-4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9PUGpCWG5EBd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1: Loading libraries"
      ],
      "metadata": {
        "id": "db8Gj_H_5yj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a9GlU4ySHfJk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8_Vw-q744-q",
        "outputId": "093a0072-622d-45b7-fa1a-ea4490e2d12e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Setup ####\n",
        "# install and import required packages\n",
        "!pip install scanpy\n",
        "#!pip install decoupler\n",
        "#!pip install omnipath\n",
        "\n",
        "import torch; torch.manual_seed(100)\n",
        "import torch.nn as nn\n",
        "import torch.utils\n",
        "import torch.distributions\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import math\n",
        "import numpy as np\n",
        "np.random.seed(100)\n",
        "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
        "import scanpy as sc\n",
        "from collections import OrderedDict\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# select the right device, depending on whether your Colab runs on GPU or CPU\n",
        "### IMPORTANT: we recommend to change your runtime to GPU, otherwise the training takes much longer\n",
        "device = 'mps'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9kSPdKN5izS",
        "outputId": "78b2dbe0-7796-407f-a1cc-66ecaea2a668"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.11.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.11.4-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: h5py>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.0)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
            "Collecting scikit-learn<1.6.0,>=1.1 (from scanpy)\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.15.3)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info2 (from scanpy)\n",
            "  Downloading session_info2-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.13.2)\n",
            "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.7)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.1->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->scanpy) (1.17.0)\n",
            "Downloading scanpy-1.11.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.11.4-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading session_info2-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading array_api_compat-1.12.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: session-info2, legacy-api-wrap, array-api-compat, scikit-learn, anndata, scanpy\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed anndata-0.11.4 array-api-compat-1.12.0 legacy-api-wrap-1.4.1 scanpy-1.11.1 scikit-learn-1.5.2 session-info2-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2: Load data"
      ],
      "metadata": {
        "id": "sVUrW5w154Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and use Scanpy to convert it into AnnData\n",
        "PBMC_train = sc.read_h5ad(\"/content/drive/MyDrive/WORK/Turing Project/Interpretable_VAE/data/PBMC_train.h5ad\")\n",
        "regulons = pd.read_csv('/content/drive/MyDrive/WORK/Turing Project/Interpretable_VAE/data/regulons.csv')"
      ],
      "metadata": {
        "id": "EdHi1Vqf56zG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# devide data into control and stimulated\n",
        "\n",
        "# Subset for a specific condition, e.g., \"control\"\n",
        "PBMC_control = PBMC_train[PBMC_train.obs[\"condition\"] == \"control\"].copy()\n",
        "\n",
        "# Another example for \"treated\"\n",
        "PBMC_stimulated = PBMC_train[PBMC_train.obs[\"condition\"] == \"stimulated\"].copy()\n"
      ],
      "metadata": {
        "id": "CaQ9KDAR59tl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3: Define model architecture"
      ],
      "metadata": {
        "id": "mQ64BMOu5-o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Encoder:\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dims, input_dims, dropout, z_dropout): #dropout between the dense layers, z_dropout define the dropout rates between the encoder/latent space\n",
        "        super(Encoder, self).__init__() #run the initialize code from nn.Module -> this class behaves like a Pytorch model\n",
        "        self.encoder = nn.Sequential(\n",
        "                                     nn.Linear(input_dims, 800),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(p = dropout),\n",
        "                                     nn.Linear(800, 800),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(p = dropout))  #two layer, fully connected encoder with dropout\n",
        "\n",
        "        # outputs mean vector u\n",
        "        self.mu = nn.Sequential(nn.Linear(800, latent_dims), # the 800 neurons in the second layers -> latent space\n",
        "                                nn.Dropout(p = z_dropout))\n",
        "        # outputs standard variance\n",
        "        self.sigma = nn.Sequential(nn.Linear(800, latent_dims),\n",
        "                                   nn.Dropout(p = z_dropout))\n",
        "\n",
        "        self.N = torch.distributions.Normal(0, 1)  # define Gaussian distribution for each input\n",
        "        self.N.loc = self.N.loc.to(device) # move to the right device\n",
        "        self.N.scale = self.N.scale.to(device)\n",
        "        self.kl = 0 # place holder for storing KL divergence (regularization term)\n",
        "        # KL measures how far the learned Gaussian is from the standard normal (0,1) -> this is a regularization term in VAE\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x) # pass the data to the encoder\n",
        "        mu =  self.mu(x) # predict mean vector\n",
        "        sigma = torch.exp(self.sigma(x)) # predict standard var exp for numeric stability\n",
        "        z = mu + sigma*self.N.sample(mu.shape)  # Sample z using reparameterization trick\n",
        "\n",
        "        self.kl = (0.5*sigma**2 + 0.5*mu**2 - torch.log(sigma) - 1/2).sum() #calculation of kullback-leibler divergence\n",
        "\n",
        "        return z # output is the sampled latent vector\n"
      ],
      "metadata": {
        "id": "fC0zmqY66Dsp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_mask(adata, regulons, add_nodes:int=10, sep = \"\\t\"):\n",
        "    \"\"\"\n",
        "    Initialize mask M that specifies which latent nodes connect to which decoder nodes.\n",
        "    Args:\n",
        "        adata (Anndata): Scanpy single-cell object, we will store the computed mask and the names of the biological processes there\n",
        "        regulons: which TFs affected which genes\n",
        "        add_nodes (int): Additional latent nodes for capturing additional variance\n",
        "    Return:\n",
        "        adata (Anndata): Scanpy single-cell object that now stores the computed mask and the names of biological processes (in the .uns[\"_vega\"] attribute)\n",
        "        mask (array): mask M that specifies whether a gene is included in the gene set of a pathway (value one) or not (value zero)\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the mask\n",
        "    # 1. Get unique genes (targets) and TFs (sources)\n",
        "    genes = regulons['target'].unique()\n",
        "    tfs = regulons['source'].unique()\n",
        "\n",
        "    # get their names and the corresponding sub‐mask\n",
        "    selected_tfs  = [\"STAT1\", \"STAT2\", \"STAT3\",\"STAT4\",\"STAT5A\",\"STAT5B\",\"STAT6\",\"IRF1\",\"IRF2\",\"IRF3\",\"IRF4\",\"IRF5\",\"IRF6\",\"IRF7\",\"IRF8\",\"IRF9\",\"NFKB\",\"AP1\",\"MYC\",\"TP53\"]\n",
        "\n",
        "    # 2. Initialize matrix M with zeros\n",
        "    M = pd.DataFrame(0, index=genes, columns=tfs)\n",
        "\n",
        "    # 3. Set M[i,j] = 1 where the gene i is affected by TF j\n",
        "    for _, row in regulons.iterrows(): # for each row in regulons\n",
        "        M.loc[row['target'], row['source']] = 1 #the corresponding genes, TF box = 1\n",
        "\n",
        "    M = M.loc[:, selected_tfs]  # if M is a pandas DataFrame\n",
        "\n",
        "    # Add unannotated nodes\n",
        "    vec = np.ones((M.shape[0], add_nodes))\n",
        "    M = np.hstack((M, vec))\n",
        "\n",
        "    adata.uns['_vega'] = dict() #create attribute \"_vega\" to store the mask and pathway information\n",
        "    adata.uns['_vega']['mask'] = M\n",
        "    adata.uns['_vega']['TFs'] = list(tfs) + ['UNANNOTATED_'+str(k) for k in range(add_nodes)]\n",
        "\n",
        "    return adata, M"
      ],
      "metadata": {
        "id": "0D6XBfC-6FPJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the create_mask function\n",
        "PBMC_control, mask_ctr = create_mask(PBMC_control,regulons , add_nodes=1)\n",
        "PBMC_stimulated, mask_sti  = create_mask(PBMC_stimulated,regulons , add_nodes=1)"
      ],
      "metadata": {
        "id": "eNACTwID6WV0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---# filter the genes in mask\n",
        "# define names to filter genes for PBMC in the next chunk\n",
        "mask_ctr_df = pd.DataFrame(mask_ctr, index= regulons.target.unique())\n",
        "# list of genes in the regulons list and in our pbmc data\n",
        "genes_mask = np.array(regulons.target.unique()) # genes in regulons list\n",
        "pbmc_genes = np.array(PBMC_train.var_names) # genes in OUR data\n",
        "# Create boolean mask of which genes are in PBMC_train\n",
        "keep = np.isin(genes_mask, pbmc_genes)\n",
        "# Apply the filter\n",
        "filtered_mask_ctr_df = mask_ctr_df.loc[keep, :]\n",
        "filtered_mask_sti_df = mask_ctr_df.loc[keep, :]\n",
        "\n",
        "# count 0 (the rows in which all genes are 0 are filtered out) (careful: have to account for the last node that are fully connected)\n",
        "# Count non-zero elements per row\n",
        "non_zero_count = np.count_nonzero(filtered_mask_ctr_df, axis=1)\n",
        "\n",
        "# Filter: keep rows with at least 2 non-zero elements\n",
        "filtered_mask_ctr_df = filtered_mask_ctr_df[non_zero_count >= 2]"
      ],
      "metadata": {
        "id": "P3bN_pLY6jJe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the genes in PBMC data\n",
        "PBMC_control_filtered = PBMC_control[:, filtered_mask_ctr_df.index].copy()\n",
        "PBMC_stimulated_filtered = PBMC_stimulated[:, filtered_mask_sti_df.index].copy()\n"
      ],
      "metadata": {
        "id": "yQrM9EXp6nPS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert pandas back to numpy array to use for downstream steps\n",
        "filtered_mask_ctr = filtered_mask_ctr_df.to_numpy()\n",
        "filtered_mask_sti = filtered_mask_sti_df.to_numpy()\n"
      ],
      "metadata": {
        "id": "brBiI_di6sSf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define VEGA's decoder\n",
        "class DecoderVEGA(nn.Module):\n",
        "  \"\"\"\n",
        "  Define VEGA's decoder (sparse, one-layer, linear, positive)\n",
        "  \"\"\"\n",
        "  def __init__(self,mask):\n",
        "        super(DecoderVEGA, self).__init__()\n",
        "\n",
        "        self.sparse_layer = nn.Sequential(SparseLayer(mask)) # we define the architecture of the decoder below with the class \"SparseLayer\"\n",
        "        # This decoder only has 1 layer (Sparse)!!!\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.sparse_layer(x.to(device))\n",
        "    return(z)\n",
        "\n",
        "# define a class SparseLayer, that specifies the decoder architecture (sparse connections based on the mask)\n",
        "class SparseLayer(nn.Module):\n",
        "  def __init__(self, mask):\n",
        "        \"\"\"\n",
        "        Extended torch.nn module which mask connection\n",
        "        \"\"\"\n",
        "        super(SparseLayer, self).__init__()\n",
        "\n",
        "        self.mask = nn.Parameter(torch.tensor(mask, dtype=torch.float).t(), requires_grad=False)\n",
        "        self.weight = nn.Parameter(torch.Tensor(mask.shape[1], mask.shape[0]))\n",
        "        self.bias = nn.Parameter(torch.Tensor(mask.shape[1]))\n",
        "        self.reset_parameters()\n",
        "\n",
        "        # mask weight\n",
        "        self.weight.data = self.weight.data * self.mask\n",
        "\n",
        "  def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, input):\n",
        "        # See the autograd section for explanation of what happens here\n",
        "        return SparseLayerFunction.apply(input, self.weight, self.bias, self.mask)\n",
        "        # OUTPUT of the decoder\n",
        "\n",
        "\n",
        "\n",
        "# defines a custom forward and backward pass\n",
        "class SparseLayerFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We define our own autograd function which masks it's weights by 'mask'.\n",
        "    For more details, see https://pytorch.org/docs/stable/notes/extending.html\n",
        "    \"\"\"\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias, mask):\n",
        "        # enforce the forward connection between latent and next layer to be sparse\n",
        "        weight = weight * mask # change weight to 0 where mask == 0\n",
        "        #calculate the output\n",
        "        output = input.mm(weight.t()) # output = input × weight.T  (torch.mm : matrix multiplication)\n",
        "        # IMPORTANT!!!\n",
        "        # input = latent: has the dim (batch_size, latent_dim)\n",
        "        #-> mask has the dim: (input_dim, latent_dim) -> transpose (latent_dim, input_dim) (input here this the original input, not the latent vector)\n",
        "        # output: (batch_size, input_dim)  = same dimension with OUR input\n",
        "        output += bias.unsqueeze(0).expand_as(output) # Add bias to all values in output\n",
        "        ctx.save_for_backward(input, weight, bias, mask)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output): # define the gradient formula\n",
        "        # compute gradient for backpropagation\n",
        "        input, weight, bias, mask = ctx.saved_tensors\n",
        "        grad_input = grad_weight = grad_bias = grad_mask = None\n",
        "\n",
        "        # These needs_input_grad checks are optional and only to improve efficiency\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output.mm(weight)\n",
        "            # grad_input: how the loss changes with respect to the input\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_output.t().mm(input) # how the loss changes with respect to weight\n",
        "            grad_weight = grad_weight * mask # change grad_weight to 0 where mask == 0  (enforce the mask even in backward pass)\n",
        "        if ctx.needs_input_grad[2]:\n",
        "            grad_bias = grad_output.sum(0).squeeze(0)\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias, grad_mask\n"
      ],
      "metadata": {
        "id": "a0BXFHNE6ydT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AaIXn1Ps62FI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4: Training\n",
        "SWA is applied on the training part"
      ],
      "metadata": {
        "id": "jpk0oxS1630-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define class that combine encoder and decoder\n",
        "class VEGA(nn.Module):\n",
        "    def __init__(self, latent_dims, input_dims, mask, dropout = 0.3, z_dropout = 0.3):\n",
        "        super(VEGA, self).__init__()\n",
        "        self.encoder = Encoder(latent_dims, input_dims, dropout, z_dropout) # we use the same encoder as before (two-layer, fully connected, non-linear)\n",
        "        self.decoder = DecoderVEGA(mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ],
      "metadata": {
        "id": "3fEYQrWk6-oY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "def trainVEGA(vae, data, epochs=30, beta = 0.0001, learning_rate = 0.01):\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr = learning_rate, weight_decay = 5e-4)\n",
        "    vae.train() #train mode\n",
        "    losses = []\n",
        "    klds = []\n",
        "    mses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_e = 0\n",
        "        kld_e = 0\n",
        "        mse_e = 0\n",
        "\n",
        "        for x in data:\n",
        "            x = x.to(device)\n",
        "            opt.zero_grad()\n",
        "            x_hat = vae(x)\n",
        "            mse = ((x - x_hat)**2).sum()\n",
        "            kld = beta* vae.encoder.kl\n",
        "            loss = mse +  kld # loss calculation\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            loss_e += loss.to('cpu').detach().numpy()\n",
        "            kld_e += kld.to('cpu').detach().numpy()\n",
        "            mse_e += mse.to('cpu').detach().numpy()\n",
        "\n",
        "        losses.append(loss_e/(len(data)*128))\n",
        "        klds.append(kld_e/(len(data)*128))\n",
        "        mses.append(mse_e/(len(data)*128))\n",
        "\n",
        "        print(\"epoch: \", epoch, \" loss: \", loss_e/(len(data)*128))\n",
        "\n",
        "    return vae, losses, klds, mses"
      ],
      "metadata": {
        "id": "93xMPUIYNCdV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader\n",
        "PBMC_controlX = torch.utils.data.DataLoader(PBMC_control_filtered.X.toarray(), batch_size=128) #set up the training data in the right format\n",
        "PBMC_stimulatedX = torch.utils.data.DataLoader(PBMC_stimulated.X.toarray(), batch_size=128) #set up the training data in the right format"
      ],
      "metadata": {
        "id": "Y9gQjPFq_E6p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# NOTE: In Runtime, Change type (hardware accelerator) -> GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WbY1J2g_Gr9",
        "outputId": "adb48432-7a89-4747-8f85-ff8455cf0a23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model for control, NO SWA\n",
        "vega_ctr = VEGA(latent_dims= filtered_mask_ctr.shape[1], input_dims = filtered_mask_ctr.shape[0], mask = filtered_mask_ctr.T, z_dropout = 0.1, dropout = 0.2).to(device) # input_dim should be the\n",
        "# model training\n",
        "vega_ctr, vega_losses_ctr, vega_klds_ctr, vega_mses_ctr = trainVEGA(vega_ctr, PBMC_controlX,epochs = 40, beta=0.00001, learning_rate=0.01) #takes about 2 mins on GPU # change beta!!!\n",
        "\n",
        "# gotta change dropout rate as well otherwise its too instable\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezlqzcx6_IvQ",
        "outputId": "a95a1004-78b6-47f9-c8cc-7843e3fba115"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  168.66997\n",
            "epoch:  1  loss:  83.2338\n",
            "epoch:  2  loss:  77.16853\n",
            "epoch:  3  loss:  74.27911\n",
            "epoch:  4  loss:  71.30065\n",
            "epoch:  5  loss:  68.666885\n",
            "epoch:  6  loss:  69.787636\n",
            "epoch:  7  loss:  69.30956\n",
            "epoch:  8  loss:  66.852806\n",
            "epoch:  9  loss:  66.3687\n",
            "epoch:  10  loss:  65.81338\n",
            "epoch:  11  loss:  65.20503\n",
            "epoch:  12  loss:  65.14792\n",
            "epoch:  13  loss:  64.781265\n",
            "epoch:  14  loss:  64.1656\n",
            "epoch:  15  loss:  64.299095\n",
            "epoch:  16  loss:  64.09797\n",
            "epoch:  17  loss:  63.42421\n",
            "epoch:  18  loss:  63.734146\n",
            "epoch:  19  loss:  63.314228\n",
            "epoch:  20  loss:  63.52697\n",
            "epoch:  21  loss:  63.820858\n",
            "epoch:  22  loss:  64.22026\n",
            "epoch:  23  loss:  63.18825\n",
            "epoch:  24  loss:  63.215977\n",
            "epoch:  25  loss:  63.324707\n",
            "epoch:  26  loss:  62.851784\n",
            "epoch:  27  loss:  62.930813\n",
            "epoch:  28  loss:  63.10973\n",
            "epoch:  29  loss:  63.174633\n",
            "epoch:  30  loss:  63.82219\n",
            "epoch:  31  loss:  62.876274\n",
            "epoch:  32  loss:  62.90719\n",
            "epoch:  33  loss:  62.73673\n",
            "epoch:  34  loss:  62.874084\n",
            "epoch:  35  loss:  62.99539\n",
            "epoch:  36  loss:  63.258778\n",
            "epoch:  37  loss:  63.020374\n",
            "epoch:  38  loss:  62.660545\n",
            "epoch:  39  loss:  62.60662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try again : selbst!!: SWA\n",
        "# loader: PBMC_controlX, opt:  Adam, model: vega_ctr , loss_fn:??\n",
        "#opt = torch.optim.Adam(vae.parameters(), lr = learning_rate, weight_decay = 5e-4)\n",
        "# https://docs.pytorch.org/docs/stable/optim.html#weight-averaging-swa-and-ema\n",
        "\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
        "\n",
        "#training loop\n",
        "def trainVEGA_SWA(vae, data, epochs=40, beta = 0.00001, learning_rate = 0.01):\n",
        "    swa_model = torch.optim.swa_utils.AveragedModel(vae)\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr = learning_rate, weight_decay = 5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt,\n",
        "                                     T_max=300)\n",
        "    vae.train() #train mode\n",
        "    losses = []\n",
        "    klds = []\n",
        "    mses = []\n",
        "    swa_start = 30\n",
        "    swa_scheduler = SWALR(opt, swa_lr=0.02) #2x faster\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_e = 0\n",
        "        kld_e = 0\n",
        "        mse_e = 0\n",
        "\n",
        "        for x in data:\n",
        "            x = x.to(device)\n",
        "            opt.zero_grad()\n",
        "            x_hat = vae(x)\n",
        "            mse = ((x - x_hat)**2).sum()\n",
        "            kld = beta* vae.encoder.kl\n",
        "            loss = mse +  kld # loss calculation\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            loss_e += loss.to('cpu').detach().numpy()\n",
        "            kld_e += kld.to('cpu').detach().numpy()\n",
        "            mse_e += mse.to('cpu').detach().numpy()\n",
        "        if epoch > swa_start:\n",
        "          swa_model.update_parameters(vae)\n",
        "          swa_scheduler.step()\n",
        "        else:\n",
        "          scheduler.step()\n",
        "\n",
        "        losses.append(loss_e/(len(data)*128))\n",
        "        klds.append(kld_e/(len(data)*128))\n",
        "        mses.append(mse_e/(len(data)*128))\n",
        "\n",
        "        print(\"epoch: \", epoch, \" loss: \", loss_e/(len(data)*128))\n",
        "    torch.optim.swa_utils.update_bn(data, swa_model)\n",
        "    return swa_model, losses, klds, mses\n"
      ],
      "metadata": {
        "id": "KNh9hyhQSpps"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model for control\n",
        "vega_ctr = VEGA(latent_dims= filtered_mask_ctr.shape[1], input_dims = filtered_mask_ctr.shape[0], mask = filtered_mask_ctr.T, z_dropout = 0.1, dropout = 0.2).to(device) # input_dim should be the\n",
        "# model training\n",
        "\n",
        "vega_ctr_swa, vega_losses_ctr, vega_klds_ctr, vega_mses_ctr = trainVEGA_SWA(vega_ctr, PBMC_controlX,epochs = 40, beta=0.00001, learning_rate=0.01) #takes about 2 mins on GPU # change beta!!!\n",
        "\n",
        "# gotta change dropout rate as well otherwise its too instable\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZhseZFWWkeT",
        "outputId": "275f369c-27b0-4cbb-a437-2dfaddbc384a"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  193.76811\n",
            "epoch:  1  loss:  85.20965\n",
            "epoch:  2  loss:  76.061386\n",
            "epoch:  3  loss:  69.54501\n",
            "epoch:  4  loss:  70.512505\n",
            "epoch:  5  loss:  71.188576\n",
            "epoch:  6  loss:  69.70807\n",
            "epoch:  7  loss:  66.24075\n",
            "epoch:  8  loss:  63.323578\n",
            "epoch:  9  loss:  62.016445\n",
            "epoch:  10  loss:  61.64245\n",
            "epoch:  11  loss:  61.08014\n",
            "epoch:  12  loss:  60.6586\n",
            "epoch:  13  loss:  60.24923\n",
            "epoch:  14  loss:  60.06552\n",
            "epoch:  15  loss:  59.793526\n",
            "epoch:  16  loss:  59.78979\n",
            "epoch:  17  loss:  59.537487\n",
            "epoch:  18  loss:  59.71161\n",
            "epoch:  19  loss:  59.543156\n",
            "epoch:  20  loss:  59.57012\n",
            "epoch:  21  loss:  59.840355\n",
            "epoch:  22  loss:  59.16455\n",
            "epoch:  23  loss:  59.02792\n",
            "epoch:  24  loss:  59.926304\n",
            "epoch:  25  loss:  59.017666\n",
            "epoch:  26  loss:  58.84915\n",
            "epoch:  27  loss:  58.797375\n",
            "epoch:  28  loss:  59.551926\n",
            "epoch:  29  loss:  59.435497\n",
            "epoch:  30  loss:  58.87618\n",
            "epoch:  31  loss:  58.687042\n",
            "epoch:  32  loss:  58.461895\n",
            "epoch:  33  loss:  93.326\n",
            "epoch:  34  loss:  60.10704\n",
            "epoch:  35  loss:  60.24049\n",
            "epoch:  36  loss:  60.362965\n",
            "epoch:  37  loss:  61.38646\n",
            "epoch:  38  loss:  60.806\n",
            "epoch:  39  loss:  61.775127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate SWA model\n",
        "- Compare SWA and non-SWA approaches by evaluating the model on test data\n",
        "- Result: In summary, applying SWA helps reducing MSE in VEGA. However, since VEGA is probabilistic, a systemical approach is to run the training multiple times, take the best model for both cases (with and without SWA), and then compare how they perform on test data."
      ],
      "metadata": {
        "id": "aGWXUUi7QXye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the data\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zHJKoU8QcQB4cLR-oICO2YY4Nu-QaZHG' -O PBMC_test.h5ad\n",
        "# load data as anndata object\n",
        "PBMC_test = sc.read_h5ad(\"PBMC_test.h5ad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD7DtgQ2LgoS",
        "outputId": "36910763-25f6-4043-f4c8-f3b3dc89f3bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-23 13:46:49--  https://docs.google.com/uc?export=download&id=1zHJKoU8QcQB4cLR-oICO2YY4Nu-QaZHG\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.141.102, 142.250.141.100, 142.250.141.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.141.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1zHJKoU8QcQB4cLR-oICO2YY4Nu-QaZHG&export=download [following]\n",
            "--2025-05-23 13:46:49--  https://drive.usercontent.google.com/download?id=1zHJKoU8QcQB4cLR-oICO2YY4Nu-QaZHG&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45277554 (43M) [application/octet-stream]\n",
            "Saving to: ‘PBMC_test.h5ad’\n",
            "\n",
            "PBMC_test.h5ad      100%[===================>]  43.18M  33.6MB/s    in 1.3s    \n",
            "\n",
            "2025-05-23 13:46:55 (33.6 MB/s) - ‘PBMC_test.h5ad’ saved [45277554/45277554]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wYLAElq4QKdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# spilt data control sti\n",
        "# Subset for a specific condition, e.g., \"control\"\n",
        "PBMC_test_ctr = PBMC_test[PBMC_test.obs[\"condition\"] == \"control\"].copy()\n",
        "\n",
        "# Another example for \"treated\"\n",
        "PBMC_test_sti = PBMC_test[PBMC_test.obs[\"condition\"] == \"stimulated\"].copy()\n"
      ],
      "metadata": {
        "id": "_kq89_5iLmgY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PBMC_test_ctr_filtered = PBMC_test_ctr[:, filtered_mask_ctr_df.index].copy()\n",
        "PBMC_test_sti_filtered = PBMC_test_sti[:, filtered_mask_ctr_df.index].copy()"
      ],
      "metadata": {
        "id": "nflZBJ1SMas6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test control loader\n",
        "PBMC_control_testX = torch.utils.data.DataLoader(PBMC_test_ctr_filtered.X.toarray(), batch_size=128) #set up the training data in the right format\n",
        "PBMC_stimulated_testX = torch.utils.data.DataLoader(PBMC_test_sti_filtered.X.toarray(), batch_size=128) #set up the training data in the right format"
      ],
      "metadata": {
        "id": "LHQI853wMDur"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluation\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.optim.swa_utils.update_bn(PBMC_controlX, vega_ctr_swa)\n",
        "vega_ctr_swa.eval()\n",
        "\n",
        "total_mse = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x in PBMC_control_testX:\n",
        "        x = x.to(device)\n",
        "        x_hat = vega_ctr_swa(x)\n",
        "        mse = F.mse_loss(x_hat, x, reduction='sum')  # or 'mean'\n",
        "        total_mse += mse.item()\n",
        "\n",
        "avg_mse = total_mse / len(PBMC_control_testX.dataset)\n",
        "print(f\"Test MSE: {avg_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D05DEHqjL91s",
        "outputId": "a7598328-ed07-45be-8a4b-13fced8d3547"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 61.2531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare to normal model\n",
        "# model evaluation\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "vega_ctr.eval()\n",
        "\n",
        "total_mse = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x in PBMC_control_testX:\n",
        "        x = x.to(device)\n",
        "        x_hat = vega_ctr(x)\n",
        "        mse = F.mse_loss(x_hat, x, reduction='sum')  # or 'mean'\n",
        "        total_mse += mse.item()\n",
        "\n",
        "avg_mse = total_mse / len(PBMC_control_testX.dataset)\n",
        "print(f\"Test MSE: {avg_mse:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJbKYq3EMtxE",
        "outputId": "aafc65c4-a312-4178-e5e1-79868dcab6f7"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 70.3493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, applying SWA helps reducing MSE in VEGA. However, since VEGA is probabilistic, a systemical approach is to run the training multiple times, take the best model for both cases (with and without SWA), and then compare how they perform on test data."
      ],
      "metadata": {
        "id": "0QvMj2jkirlJ"
      }
    }
  ]
}