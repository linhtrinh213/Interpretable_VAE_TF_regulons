{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G53m43iNX0W3"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrOlE5l8LC8KkoyZfZJQ2M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linhtrinh213/Interpretable_VAE_TF_regulons/blob/main/base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qbro_moXN4x"
      },
      "outputs": [],
      "source": [
        "### Note\n",
        "To work productively with gg colab (12 hours session):\n",
        "1. Mount gg drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "2. Then save or load files: model.save('/content/drive/MyDrive/my_model.h5')\n",
        "\n",
        "- Export code (notebook) frequently (or save a copy in github)\n",
        "- Save model checkpoints/variables...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do next:\n",
        "1. choose 10 real TFs\n",
        "2. train 2 base models (one for the control and one for the stimulated)\n",
        "(-> observe the weights)\n",
        "3. retrain many times"
      ],
      "metadata": {
        "id": "XRCgQS2cwBBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1: Loading neccessities"
      ],
      "metadata": {
        "id": "G53m43iNX0W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRlc8vLfXnRT",
        "outputId": "24258b98-59b9-4ac3-8c19-865d9338e3c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Setup ####\n",
        "# install and import required packages\n",
        "!pip install scanpy\n",
        "!pip install decoupler\n",
        "!pip install omnipath\n",
        "\n",
        "import torch; torch.manual_seed(100)\n",
        "import torch.nn as nn\n",
        "import torch.utils\n",
        "import torch.distributions\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import math\n",
        "import numpy as np\n",
        "np.random.seed(100)\n",
        "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
        "import scanpy as sc\n",
        "from collections import OrderedDict\n",
        "from collections import Counter\n",
        "\n",
        "# select the right device, depending on whether your Colab runs on GPU or CPU\n",
        "### IMPORTANT: we recommend to change your runtime to GPU, otherwise the training takes much longer\n",
        "device = 'mps'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0kkAyARXpnu",
        "outputId": "971c681c-b9e5-4ca8-c66c-499a38c7d19c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.11.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.11.4-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: h5py>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.13.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.5.0)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (24.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
            "Collecting scikit-learn<1.6.0,>=1.1 (from scanpy)\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.15.3)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info2 (from scanpy)\n",
            "  Downloading session_info2-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.13.2)\n",
            "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.7)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.1->scanpy) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->scanpy) (1.17.0)\n",
            "Downloading scanpy-1.11.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.11.4-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading session_info2-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading array_api_compat-1.11.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: session-info2, legacy-api-wrap, array-api-compat, scikit-learn, anndata, scanpy\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed anndata-0.11.4 array-api-compat-1.11.2 legacy-api-wrap-1.4.1 scanpy-1.11.1 scikit-learn-1.5.2 session-info2-0.1.2\n",
            "Collecting decoupler\n",
            "  Downloading decoupler-1.9.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting numba<0.62.0,>=0.61.0 (from decoupler)\n",
            "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy<3,>=2 in /usr/local/lib/python3.11/dist-packages (from decoupler) (2.0.2)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from decoupler) (2.2.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.4 in /usr/local/lib/python3.11/dist-packages (from decoupler) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from decoupler) (4.13.2)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba<0.62.0,>=0.61.0->decoupler)\n",
            "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.2->decoupler) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.2->decoupler) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.2->decoupler) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.2->decoupler) (1.17.0)\n",
            "Downloading decoupler-1.9.2-py3-none-any.whl (122 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.6/122.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: llvmlite, numba, decoupler\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed decoupler-1.9.2 llvmlite-0.44.0 numba-0.61.2\n",
            "Collecting omnipath\n",
            "  Downloading omnipath-1.0.9-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: attrs>=20.2.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (25.3.0)\n",
            "Collecting docrep>=0.3.1 (from omnipath)\n",
            "  Downloading docrep-0.3.2.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: inflect>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (7.5.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from omnipath) (24.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.51.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from omnipath) (4.13.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from omnipath) (1.17.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from docrep>=0.3.1->omnipath) (1.17.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect>=4.1.0->omnipath) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect>=4.1.0->omnipath) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->omnipath) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->omnipath) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->omnipath) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->omnipath) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->omnipath) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->omnipath) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.24.0->omnipath) (2025.4.26)\n",
            "Downloading omnipath-1.0.9-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docrep\n",
            "  Building wheel for docrep (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docrep: filename=docrep-0.3.2-py3-none-any.whl size=19876 sha256=a6690d221c27ed97059c22d7ec59720124d2e9294a567ee98c06e692f8271c2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/76/8f/0ecb7d357c0bff71a2bd1940671be2d07a200752da9189bb55\n",
            "Successfully built docrep\n",
            "Installing collected packages: docrep, omnipath\n",
            "Successfully installed docrep-0.3.2 omnipath-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2: The data"
      ],
      "metadata": {
        "id": "RfRDtpy8YF7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and use Scanpy to convert it into AnnData\n",
        "PBMC_train = sc.read_h5ad(\"/content/drive/MyDrive/WORK/Turing Project/Interpretable_VAE/data/PBMC_train.h5ad\")\n",
        "regulons.to_csv('/content/drive/MyDrive/WORK/Turing Project/Interpretable_VAE/data/regulons.csv')\n"
      ],
      "metadata": {
        "id": "AEqMHRTUX78r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the data\n",
        "print(PBMC_train) #the data is stored as an anndata object\n",
        "print(Counter(PBMC_train.obs[\"cell_type\"])) #summary of cell types\n",
        "print(Counter(PBMC_train.obs[\"condition\"])) #summary of conditions\n",
        "# 13515 observation: cells (.obs attribute)\n",
        "# gene expression data: .X (sparse matrix)\n",
        "# annotations for the variables (genes) in the .var attribute\n",
        "# the attribute have specific features that can be accessed using squared bracktets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-abvgEnX94Q",
        "outputId": "5324fe93-d22d-4c0c-f826-814861acb459"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnnData object with n_obs × n_vars = 13515 × 6998\n",
            "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
            "    var: 'gene_symbol', 'n_cells'\n",
            "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
            "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
            "    obsp: 'connectivities', 'distances'\n",
            "Counter({'CD4T': 4452, 'FCGR3A+Mono': 2881, 'CD14+Mono': 2049, 'B': 1448, 'NK': 931, 'CD8T': 892, 'Dendritic': 862})\n",
            "Counter({'stimulated': 7109, 'control': 6406})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# devide data into control and stimulated\n",
        "\n",
        "# Subset for a specific condition, e.g., \"control\"\n",
        "PBMC_control = PBMC_train[PBMC_train.obs[\"condition\"] == \"control\"].copy()\n",
        "\n",
        "# Another example for \"treated\"\n",
        "PBMC_stimulated = PBMC_train[PBMC_train.obs[\"condition\"] == \"stimulated\"].copy()\n"
      ],
      "metadata": {
        "id": "H9y5UdLFX_p6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Encoder:\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dims, input_dims, dropout, z_dropout): #dropout between the dense layers, z_dropout define the dropout rates between the encoder/latent space\n",
        "        super(Encoder, self).__init__() #run the initialize code from nn.Module -> this class behaves like a Pytorch model\n",
        "        self.encoder = nn.Sequential(\n",
        "                                     nn.Linear(input_dims, 800),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(p = dropout),\n",
        "                                     nn.Linear(800, 800),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Dropout(p = dropout))  #two layer, fully connected encoder with dropout\n",
        "\n",
        "        # outputs mean vector u\n",
        "        self.mu = nn.Sequential(nn.Linear(800, latent_dims), # the 800 neurons in the second layers -> latent space\n",
        "                                nn.Dropout(p = z_dropout))\n",
        "        # outputs standard variance\n",
        "        self.sigma = nn.Sequential(nn.Linear(800, latent_dims),\n",
        "                                   nn.Dropout(p = z_dropout))\n",
        "\n",
        "        self.N = torch.distributions.Normal(0, 1)  # define Gaussian distribution for each input\n",
        "        self.N.loc = self.N.loc.to(device) # move to the right device\n",
        "        self.N.scale = self.N.scale.to(device)\n",
        "        self.kl = 0 # place holder for storing KL divergence (regularization term)\n",
        "        # KL measures how far the learned Gaussian is from the standard normal (0,1) -> this is a regularization term in VAE\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x) # pass the data to the encoder\n",
        "        mu =  self.mu(x) # predict mean vector\n",
        "        sigma = torch.exp(self.sigma(x)) # predict standard var exp for numeric stability\n",
        "        z = mu + sigma*self.N.sample(mu.shape)  # Sample z using reparameterization trick\n",
        "\n",
        "        self.kl = (0.5*sigma**2 + 0.5*mu**2 - torch.log(sigma) - 1/2).sum() #calculation of kullback-leibler divergence\n",
        "\n",
        "        return z # output is the sampled latent vector\n"
      ],
      "metadata": {
        "id": "R3HwOJYXYKwQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3: regulons and create the mask"
      ],
      "metadata": {
        "id": "g-E70Ik0YZUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To achieve interpretability through model's decoder:\n",
        "1. one-layer decoder: every latent var is directly connected to the output var\n",
        "2. linear decoder: no activation function in the decoder\n",
        "3. positive decoder: restrict the decoder weights to positive values (all negative weights = 0) (*why not - weight?, cant it be for inhibition?)\n",
        "=> for this project: TF: It can be negative!\n",
        "4. sparse decoder: latent node = TF, which only connect to certain genes\n",
        "\n",
        "\n",
        "- How do we do that?\n",
        "++ mask M: define decoder connections. Rows = genes. Columns = TF. If TF j has an effect on gene i -> the corresponding entry in the matrix M(i,j) contains the value 1. ??: has an effect = activate AND inhibit, or only activate?\n",
        "So mask M: only defines which latent node **connect** to which decoder node, and the weights will learn + or - (activation or inhibition, or at least thats what we expect).\n",
        "++ mask M: also includes fully connected nodes (all values = 1) -> for better data reconstruction and compensates for connections that are missing"
      ],
      "metadata": {
        "id": "tClVmrKyYhlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# processed regulons\n",
        "import decoupler as dc\n",
        "regulons = dc.get_collectri(organism='human', split_complexes=False)\n",
        "# CollecTRI netork: a curated collection of TFs and their transcriptional targets\n",
        "# spilt_complexes: keep or spilt complexes into subunits\n",
        "# weight: 1 is activation. -1 is inhibition\n",
        "# raw regulons\n",
        "#import omnipath as op\n",
        "#op.interactions.CollecTRI.get(genesymbols=True, organism=9606L, loops=True)#"
      ],
      "metadata": {
        "id": "zn4DHMDdYiS4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regulons.target.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnp_7q6HilF_",
        "outputId": "ffdd1914-5135-418a-bbd1-83b83152f8dc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<StringArray>\n",
              "[    'BAX',    'BCL2',    'BCL6',   'CCND2',  'CDKN1A',    'CSF1',   'FOXO3',\n",
              "     'JUN',    'PIM1',    'TP53',\n",
              " ...\n",
              "  'RNF114',   'ETNK1',    'CHL1',    'DSC1', 'GPRASP1',   'MAGI2',  'REC114',\n",
              "   'SYNJ1',   'TCP11', 'PIK3AP1']\n",
              "Length: 6627, dtype: string"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_mask(adata, regulons, add_nodes:int=10, sep = \"\\t\"):\n",
        "    \"\"\"\n",
        "    Initialize mask M that specifies which latent nodes connect to which decoder nodes.\n",
        "    Args:\n",
        "        adata (Anndata): Scanpy single-cell object, we will store the computed mask and the names of the biological processes there\n",
        "        regulons: which TFs affected which genes\n",
        "        add_nodes (int): Additional latent nodes for capturing additional variance\n",
        "    Return:\n",
        "        adata (Anndata): Scanpy single-cell object that now stores the computed mask and the names of biological processes (in the .uns[\"_vega\"] attribute)\n",
        "        mask (array): mask M that specifies whether a gene is included in the gene set of a pathway (value one) or not (value zero)\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the mask\n",
        "    # 1. Get unique genes (targets) and TFs (sources)\n",
        "    genes = regulons['target'].unique()\n",
        "    tfs = regulons['source'].unique()\n",
        "    # randomly choose 10 distinct TF‐indices\n",
        "    selected_idx = np.random.choice(len(tfs), size=10, replace=False)\n",
        "\n",
        "    # get their names and the corresponding sub‐mask\n",
        "    selected_tfs  = [tfs[i]   for i in selected_idx]\n",
        "\n",
        "    # 2. Initialize matrix M with zeros\n",
        "    M = pd.DataFrame(0, index=genes, columns=tfs)\n",
        "\n",
        "    # 3. Set M[i,j] = 1 where the gene i is affected by TF j\n",
        "    for _, row in regulons.iterrows(): # for each row in regulons\n",
        "        M.loc[row['target'], row['source']] = 1 #the corresponding genes, TF box = 1\n",
        "\n",
        "    M = M.iloc[:, selected_idx]  # if M is a pandas DataFrame\n",
        "\n",
        "    # Add unannotated nodes\n",
        "    vec = np.ones((M.shape[0], add_nodes))\n",
        "    M = np.hstack((M, vec))\n",
        "\n",
        "    adata.uns['_vega'] = dict() #create attribute \"_vega\" to store the mask and pathway information\n",
        "    adata.uns['_vega']['mask'] = M\n",
        "    adata.uns['_vega']['TFs'] = list(tfs) + ['UNANNOTATED_'+str(k) for k in range(add_nodes)]\n",
        "\n",
        "    return adata, M"
      ],
      "metadata": {
        "id": "Faud-u0jbmbr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the create_mask function\n",
        "PBMC_control, mask_ctr = create_mask(PBMC_control,regulons , add_nodes=1)\n",
        "PBMC_stimulated, mask_sti  = create_mask(PBMC_stimulated,regulons , add_nodes=1)"
      ],
      "metadata": {
        "id": "TsxFGGd1Yo-J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mask_ctr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUZgw8DhfFNV",
        "outputId": "da5f61c3-ce72-4fdc-b919-02b690f1affd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 1. 1.]\n",
            " [0. 0. 1. ... 0. 1. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4: Decoder\n"
      ],
      "metadata": {
        "id": "JugSSfONfRWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filter the genes\n",
        "genes_mask = np.array(regulons.target.unique()) # genes in regulons list\n",
        "\n",
        "pbmc_genes = np.array(PBMC_train.var_names)# genes in OUR data\n",
        "\n",
        "# Create boolean mask of which genes are in PBMC_train\n",
        "keep = np.isin(genes_mask, pbmc_genes)\n",
        "\n",
        "# Apply the filter\n",
        "filtered_mask_ctr = mask_ctr[keep, :]\n",
        "filtered_mask_sti = mask_sti[keep, :]"
      ],
      "metadata": {
        "id": "x3zK1TAcfVn5"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_mask_ctr.shape #2k genes, 11 (10 + 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwsNA6LsivcR",
        "outputId": "dd28f7b2-2499-4ebc-b578-8886ee5f77a8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2168, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ffwq9X2-liaG"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DP22OxWak1UL"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gene_keep = genes_mask[keep]\n",
        "PBMC_control_filtered = PBMC_control[:, PBMC_control.var_names.isin(gene_keep)].copy()\n"
      ],
      "metadata": {
        "id": "3iPHZ9Hnk_5a"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PBMC_control_filtered # 2k genes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYKnoOGCl_IQ",
        "outputId": "0cfa326e-93d9-47c9-9e94-c00ae509be1f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AnnData object with n_obs × n_vars = 6406 × 2168\n",
              "    obs: 'condition', 'n_counts', 'n_genes', 'mt_frac', 'cell_type'\n",
              "    var: 'gene_symbol', 'n_cells'\n",
              "    uns: 'cell_type_colors', 'condition_colors', 'neighbors'\n",
              "    obsm: 'X_pca', 'X_tsne', 'X_umap'\n",
              "    obsp: 'connectivities', 'distances'"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define VEGA's decoder\n",
        "\n",
        "class DecoderVEGA(nn.Module):\n",
        "  \"\"\"\n",
        "  Define VEGA's decoder (sparse, one-layer, linear, positive)\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               mask):\n",
        "        super(DecoderVEGA, self).__init__()\n",
        "\n",
        "        self.sparse_layer = nn.Sequential(SparseLayer(mask)) # we define the architecture of the decoder below with the class \"SparseLayer\"\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.sparse_layer(x.to(device))\n",
        "    return(z)\n",
        "\n",
        "# define a class SparseLayer, that specifies the decoder architecture (sparse connections based on the mask)\n",
        "class SparseLayer(nn.Module):\n",
        "  def __init__(self, mask):\n",
        "        \"\"\"\n",
        "        Extended torch.nn module which mask connection\n",
        "        \"\"\"\n",
        "        super(SparseLayer, self).__init__()\n",
        "\n",
        "        self.mask = nn.Parameter(torch.tensor(mask, dtype=torch.float).t(), requires_grad=False)\n",
        "        self.weight = nn.Parameter(torch.Tensor(mask.shape[1], mask.shape[0]))\n",
        "        self.bias = nn.Parameter(torch.Tensor(mask.shape[1]))\n",
        "        self.reset_parameters()\n",
        "\n",
        "        # mask weight\n",
        "        self.weight.data = self.weight.data * self.mask\n",
        "\n",
        "  def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, input):\n",
        "        # See the autograd section for explanation of what happens here\n",
        "        return SparseLayerFunction.apply(input, self.weight, self.bias, self.mask)\n",
        "\n",
        "\n",
        "######### You don't need to understand this part of the code in detail #########\n",
        "class SparseLayerFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We define our own autograd function which masks it's weights by 'mask'.\n",
        "    For more details, see https://pytorch.org/docs/stable/notes/extending.html\n",
        "    \"\"\"\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias, mask):\n",
        "\n",
        "        weight = weight * mask # change weight to 0 where mask == 0\n",
        "        #calculate the output\n",
        "        output = input.mm(weight.t())\n",
        "        output += bias.unsqueeze(0).expand_as(output) # Add bias to all values in output\n",
        "        ctx.save_for_backward(input, weight, bias, mask)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output): # define the gradient formula\n",
        "        input, weight, bias, mask = ctx.saved_tensors\n",
        "        grad_input = grad_weight = grad_bias = grad_mask = None\n",
        "\n",
        "        # These needs_input_grad checks are optional and only to improve efficiency\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output.mm(weight)\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_output.t().mm(input)\n",
        "            # change grad_weight to 0 where mask == 0\n",
        "            grad_weight = grad_weight * mask\n",
        "        if ctx.needs_input_grad[2]:\n",
        "            grad_bias = grad_output.sum(0).squeeze(0)\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias, grad_mask\n"
      ],
      "metadata": {
        "id": "rXV5F3YHjF-R"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4: Combine encoder and decoder"
      ],
      "metadata": {
        "id": "HOEhaeMEjM-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define class that combine encoder and decoder\n",
        "class VEGA(nn.Module):\n",
        "    def __init__(self, latent_dims, input_dims, mask, dropout = 0.3, z_dropout = 0.3):\n",
        "        super(VEGA, self).__init__()\n",
        "        self.encoder = Encoder(latent_dims, input_dims, dropout, z_dropout) # we use the same encoder as before (two-layer, fully connected, non-linear)\n",
        "        self.decoder = DecoderVEGA(mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)"
      ],
      "metadata": {
        "id": "t4uQC1fhjKoN"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "def trainVEGA(vae, data, epochs=50, beta = 0.0001, learning_rate = 0.01):\n",
        "    opt = torch.optim.Adam(vae.parameters(), lr = learning_rate, weight_decay = 5e-4)\n",
        "    vae.train() #train mode\n",
        "    losses = []\n",
        "    klds = []\n",
        "    mses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        loss_e = 0\n",
        "        kld_e = 0\n",
        "        mse_e = 0\n",
        "\n",
        "        for x in data:\n",
        "            x = x.to(device)\n",
        "            opt.zero_grad()\n",
        "            x_hat = vae(x)\n",
        "            mse = ((x - x_hat)**2).sum()\n",
        "            kld = beta* vae.encoder.kl\n",
        "            loss = mse +  kld # loss calculation\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            loss_e += loss.to('cpu').detach().numpy()\n",
        "            kld_e += kld.to('cpu').detach().numpy()\n",
        "            mse_e += mse.to('cpu').detach().numpy()\n",
        "\n",
        "        losses.append(loss_e/(len(data)*128))\n",
        "        klds.append(kld_e/(len(data)*128))\n",
        "        mses.append(mse_e/(len(data)*128))\n",
        "\n",
        "        print(\"epoch: \", epoch, \" loss: \", loss_e/(len(data)*128))\n",
        "\n",
        "    return vae, losses, klds, mses"
      ],
      "metadata": {
        "id": "7sRldAVXjSKZ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model for control\n",
        "PBMC_controlX = torch.utils.data.DataLoader(PBMC_control_filtered.X.toarray(), batch_size=128) #set up the training data in the right format\n",
        "PBMC_stimulatedX = torch.utils.data.DataLoader(PBMC_stimulated.X.toarray(), batch_size=128) #set up the training data in the right format\n",
        "\n"
      ],
      "metadata": {
        "id": "gExBaoxajeZV"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# NOTE: In Runtime, Change type (hardware accelerator) -> GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnWMkHFajp0M",
        "outputId": "6de3f04e-caee-43d7-9b7e-4261035b97d0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model for control\n",
        "vega_ctr = VEGA(latent_dims= filtered_mask_ctr.shape[1], input_dims = filtered_mask_ctr.shape[0], mask = filtered_mask_ctr.T, z_dropout = 0.1, dropout = 0.1).to(device) # input_dim should be the\n",
        "\n",
        "# latent_dims should be number of TFs\n",
        "# input dims = PBMC_train.shape[1]??\n",
        "# model training\n",
        "vega_ctr, vega_losses_ctr, vega_klds_ctr, vega_mses_ctr = trainVEGA(vega_ctr, PBMC_controlX,epochs = 50, beta = 0.00001) #takes about 2 mins on GPU # change beta!!!\n",
        "# gotta change dropout rate as well otherwise its too instable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znxF12GajTG0",
        "outputId": "b2ab0ba3-7604-4563-d090-7e0631c3b9e0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0  loss:  429.96216\n",
            "epoch:  1  loss:  159.67598\n",
            "epoch:  2  loss:  136.07414\n",
            "epoch:  3  loss:  123.63394\n",
            "epoch:  4  loss:  117.009125\n",
            "epoch:  5  loss:  113.12189\n",
            "epoch:  6  loss:  113.404816\n",
            "epoch:  7  loss:  109.70375\n",
            "epoch:  8  loss:  108.13356\n",
            "epoch:  9  loss:  106.626976\n",
            "epoch:  10  loss:  103.705635\n",
            "epoch:  11  loss:  101.3185\n",
            "epoch:  12  loss:  98.31986\n",
            "epoch:  13  loss:  96.75497\n",
            "epoch:  14  loss:  96.3989\n",
            "epoch:  15  loss:  96.61102\n",
            "epoch:  16  loss:  94.474045\n",
            "epoch:  17  loss:  94.14765\n",
            "epoch:  18  loss:  93.84986\n",
            "epoch:  19  loss:  93.33594\n",
            "epoch:  20  loss:  92.97582\n",
            "epoch:  21  loss:  93.097084\n",
            "epoch:  22  loss:  92.63841\n",
            "epoch:  23  loss:  92.68052\n",
            "epoch:  24  loss:  92.26901\n",
            "epoch:  25  loss:  92.31285\n",
            "epoch:  26  loss:  92.646324\n",
            "epoch:  27  loss:  92.810776\n",
            "epoch:  28  loss:  93.55039\n",
            "epoch:  29  loss:  92.31476\n",
            "epoch:  30  loss:  92.50665\n",
            "epoch:  31  loss:  93.0506\n",
            "epoch:  32  loss:  89.37131\n",
            "epoch:  33  loss:  89.00345\n",
            "epoch:  34  loss:  88.98653\n",
            "epoch:  35  loss:  89.11402\n",
            "epoch:  36  loss:  88.92289\n",
            "epoch:  37  loss:  88.98437\n",
            "epoch:  38  loss:  89.48859\n",
            "epoch:  39  loss:  88.74129\n",
            "epoch:  40  loss:  89.10773\n",
            "epoch:  41  loss:  89.88248\n",
            "epoch:  42  loss:  89.099785\n",
            "epoch:  43  loss:  88.50124\n",
            "epoch:  44  loss:  88.984436\n",
            "epoch:  45  loss:  88.54948\n",
            "epoch:  46  loss:  89.04744\n",
            "epoch:  47  loss:  88.57201\n",
            "epoch:  48  loss:  88.70861\n",
            "epoch:  49  loss:  88.705956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hsjRznWsvpx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZLfO30UrjS4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error: mat1xmat2 dimensions (mask have 6k7 genes. our data have 6k9 genes -> at some point they will not be compatible)\n",
        "-> How to fix it:\n",
        "\n",
        "1. For the mask: which gene in the in the regulon list is in OUR data (for now: around 2k genes)\n",
        "Only keep those 2k genes for the PBMC as well !!\n",
        "\n",
        "2. Plus, choose only around 10 TFs so that training faster."
      ],
      "metadata": {
        "id": "Bpy4LSMktFcQ"
      }
    }
  ]
}